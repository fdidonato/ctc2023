{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyM6Fk5bLlfV74OSbuhUMcqw"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["**BREAST CANCER : Classificazione binaria**\n","\n","Questo esercizio ci permetterà di creare una rete neurale ion grado di predire, date le dimensioni delle parti che costituiscono un tumore al seno, se questo è un tumore benigno o maligno."],"metadata":{"id":"y7TBRLPIZRn-"}},{"cell_type":"code","source":["!pip install pandas\n","!pip install numpy\n","!pip install matplotlib\n","!pip install sklearn\n","!pip install keras"],"metadata":{"id":"FpbHRd3Dy25f"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Importazione dei moduli necessari**\n","\n","Come prima cosa, nel nostro programma python effettuiamo le import delle dipendenze necessarie"],"metadata":{"id":"HQNCjFKc0Icz"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"dh5pYRdFylXR"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn.preprocessing import StandardScaler\n","\n","from keras.models import Sequential\n","from keras.layers import Dense"]},{"cell_type":"markdown","source":["**Download del dataframe**\n","\n","in questo caso il dataframe è senza colonne. Durante la creazione possiamo specificare noi i nomi delle colonne passandole come parametro alla nostra function pandas"],"metadata":{"id":"eFtvSwPVKlSc"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"I6EQLg7_ZHKh"},"outputs":[],"source":["breast_cancer = pd.read_csv(\"breastcancer.csv\");\n","dataset.head()"]},{"cell_type":"markdown","source":["Vediamo che la colonna che contiene le nostre targets, ovvero i valori che vogliamo predire, si chiama diagnosis. Contiamo quanti sono i valori univoci nella colonna per capire con quante classi abbiamo a che fare"],"metadata":{"id":"NIbT-XcQK1_V"}},{"cell_type":"code","source":["targets = dataset[\"diagnosis\"].unique().tolist();\n","targets"],"metadata":{"id":"3Xl49Q7BczGs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["le classi sono solamente due. Possiamo quindi considerare come colonna dei risultati un unica colonna che contiene 1 se il tumore è maligno e 0 altrimenti (tumore benigno)"],"metadata":{"id":"ULNnX1FFLBoc"}},{"cell_type":"markdown","source":["**Creiamo degli array numpy**\n","\n","La libreria Keras, che serve per creare le reti neurali, lavora però con un formato di dati particolare. Gli array Numpy. Potete immaginarli come degli array ndimensionali, una sorta di matrici. Quello che dobbiamo fare quindi sarà convertire il nostro dataframe in due array numpy : uno per le features e uno per le labels"],"metadata":{"id":"kKPwF4SQ4Ctc"}},{"cell_type":"code","source":["X = dataset.drop('diagnosis',axis=1); # creiamo l'array numpy delle features droppando dal dataset l'ultima colonna che contiene le labels\n","Y = dataset['diagnosis'].values; # creiamo l'array numpy prendendo dal dataset solamente la colonna che contiene le labels"],"metadata":{"id":"JIOLqwyc4Y2X"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Vediamo che i numeri contenuti nelle colonne del dataset assumono valori in un range veramente molto ampio. Per massimizzare le nostre prestazioni in addestramento e non ricorrere in problemi di discesa del gradiente, standardizziamo tutti i nostri valori con uno scaler standard\n"],"metadata":{"id":"pBP7TX3CLQeD"}},{"cell_type":"code","source":["sc = StandardScaler()\n","X = sc.fit_transform(X)"],"metadata":{"id":"CeW1Y4P47LQJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["effettuiamo il LabelEncoding quindi della colonna dei risultati in modo da avere un mapping 0,1"],"metadata":{"id":"CZjvOzjsLgg6"}},{"cell_type":"code","source":["from sklearn.preprocessing import LabelEncoder\n","le = LabelEncoder()\n","Y_le = le.fit_transform(Y)\n","Y_mapping = dict(zip(le.classes_, range(0, len(le.classes_)+1)))\n","print(Y_mapping)"],"metadata":{"id":"fT8Scb6d5RIi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Addestramento**\n","\n","ora che abbiamo i nostri due arrays numpy preparati a dovere, uno per le features e uno per le labels, procediamo a creare la nostra rete neurale usando keras. Prima di tutto dai nostri array numpy estraiamo un subset di dati per l'addestramento e uno per i test, per vedere effettivamente se la nostra rete neurale funziona\n"],"metadata":{"id":"JqWK1Yf6IpkH"}},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","X_train, X_test, Y_train, Y_test = train_test_split(X,Y_le, test_size=0.2,random_state = 0);"],"metadata":{"id":"tzaRMHsXI_Vn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["adesso creiamo la nostra rete neurale strato per strato e addestriamola. Durante l'addestramento vedremo gradualmente scendere il valore di loss, che sarebbe il valore della nostra funzione di errore (sempre se tutto è andato bene !) e dovremmo veder aumentare anche il valore dell'accuratezza del risultato"],"metadata":{"id":"nyBrTbw5JAO5"}},{"cell_type":"code","source":["model = Sequential()\n","model.add(Dense(12,input_dim=X_train.shape[1],activation=\"relu\"))\n","model.add(Dense(8,activation=\"relu\"))\n","model.add(Dense(4,activation=\"relu\"))\n","model.add(Dense(1,activation=\"sigmoid\"))\n","model.compile(loss='binary_crossentropy', optimizer='sgd', metrics=['accuracy'])\n","\n","history = model.fit(X_train,Y_train,epochs=200)"],"metadata":{"id":"OYkzxCBI6A9r"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Abbiamo adesso il modello addestrato e nella variabile history la storia della nostra funzione di errore. Possiamo graficarla per vedere com'è stato l'andamento dell'errore dall'inizio dell'addestramento fino alla fine. Inoltre potremmo vedere anche l'andamento dell'accuratezza durante le epoche\n"],"metadata":{"id":"y6lF5SRCL-rx"}},{"cell_type":"code","source":["# grafico della loss function durante le epoche\n","plt.plot(history.history['loss'])\n","plt.title('model loss')\n","plt.ylabel('loss')\n","plt.xlabel('epoch')\n","plt.legend(['Train'], loc='upper right')\n","plt.show()\n","\n","# grafico dell'accuratezza durante le epoche\n","plt.plot(history.history['accuracy'])\n","plt.title('model accuracy')\n","plt.ylabel('accuracy')\n","plt.xlabel('epoch')\n","plt.legend(['Train'], loc='upper right')\n","plt.show()"],"metadata":{"id":"Y2RS2rLRKW-q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["loss, acc = model.evaluate(X_test, Y_test)\n","print(\"Loss sul test set: %.4f\" % loss)\n","print(\"Accuracy sul test set: %.4f\" % acc)"],"metadata":{"id":"dq1tsaMJICFg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["Y_pred = model.predict(X_test)\n","Y_test"],"metadata":{"id":"ZKw-Ap4QFz1J"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["Y_pred"],"metadata":{"id":"y46IkexhJbX4"},"execution_count":null,"outputs":[]}]}